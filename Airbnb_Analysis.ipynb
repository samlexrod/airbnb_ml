{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CRISP-DM Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I will be following the CRISP-DM (Cross-industry standard process for data mining) process.\n",
    "\n",
    "![alt text](https://codesachin.files.wordpress.com/2015/09/crisp-dm.gif)\n",
    "The process follows these steps:\n",
    "    1. Pick a dataset\n",
    "    2. Generate business questions\n",
    "    3. Obtain data understanding\n",
    "    4. Prepare the data\n",
    "    5. Analyze, Model, and Visualize\n",
    "    6. Evaluate your analysis\n",
    "    7. Communicate business insights   \n",
    " \n",
    " \n",
    "    \n",
    "This project approaches these steps in the following way:\n",
    "    1. Boston & Seattle Airbnb Open Data \n",
    "    2. I am starting with these business questions.\n",
    "        a. What features influence more listing prices?\n",
    "        b. Which type of property has a higher price in general?\n",
    "        c. How well can we provide a suggested price to a new host's listing?\n",
    "    3. Steps 3 to 6 will be conducted using a Jupyter notebook, and some Python 3 magic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process Character Legend**:\n",
    "During the analysis, we will be conducting a couple of processess designed to clean or transform the data. In order to organize the process the most, we are flagging the tittle or subtitle of the process with the following legend:\n",
    "\n",
    "* **RC** - Removed Columns\n",
    "* **RR** - Removed Rows\n",
    "* **T** - Transformation\n",
    "* **E** - Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be covering the following subjects in this project:\n",
    "* Extracting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from helper import unzip_files, read_concat\n",
    "from helper import cleaning_dollar\n",
    "from helper import cleaning_percent\n",
    "from helper import AnalysisStatus\n",
    "from helper import DummySplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from wordcloud import WordCloud\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for Extract\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boston-airbnb-open-data.zip',\n",
       " 'boston_airbnb_open_data',\n",
       " 'extra_seattle',\n",
       " 'seattle',\n",
       " 'seattle.zip']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting zip file names\n",
    "data_zip = os.listdir('data')\n",
    "data_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already extracted!\n",
      "File already extracted!\n",
      "File already extracted!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data\\\\boston_airbnb_open_data', 'data\\\\seattle']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting datasets\n",
    "folder_names = unzip_files(data_zip)\n",
    "folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['calendar.csv', 'listings.csv', 'reviews.csv'],\n",
       " ['calendar.csv', 'listings.csv', 'reviews.csv']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[os.listdir(folder_name) for folder_name in folder_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Calendar Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = read_concat(folder_names, 'calendar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of calendar features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "calendar.groupby('rowsource').count().plot.barh();\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left');\n",
    "for x in calendar.rowsource.value_counts().values:\n",
    "    plt.vlines(x, -1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Between Nulls Prices and Availability of Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>price</th>\n",
       "      <th>rowsource</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>available</th>\n",
       "      <th>null_price</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <th>True</th>\n",
       "      <td>1124881</td>\n",
       "      <td>1124881</td>\n",
       "      <td>0</td>\n",
       "      <td>1124881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <th>False</th>\n",
       "      <td>1577579</td>\n",
       "      <td>1577579</td>\n",
       "      <td>1577579</td>\n",
       "      <td>1577579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date  listing_id    price  rowsource\n",
       "available null_price                                         \n",
       "f         True        1124881     1124881        0    1124881\n",
       "t         False       1577579     1577579  1577579    1577579"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar.assign(null_price=lambda x: (x.price.apply(type) == float)).groupby(\n",
    "    ['available', 'null_price']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** Only price is missing values. However, we can notice that there is a relationship between the null values and the listing not being available. This suggests that all nulls are actually zero values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Listings Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = read_concat(folder_names, 'listings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of listings features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.groupby('rowsource').count().plot.bar(\n",
    "    legend=False, figsize=(14, 5), rot=0, width=.9)\n",
    "for x in listings.rowsource.value_counts().values:\n",
    "    plt.hlines(x, -1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Distribution of listings Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_nulls = listings.isnull().sum().where(lambda x: x > 0).dropna(\n",
    ").sort_values(ascending=True)\n",
    "\n",
    "listing_nulls.plot.barh(\n",
    "    figsize=(10, 10), title=\"Missing Value Counts\");\n",
    "plt.vlines(listings.shape[0], 100, 0);\n",
    "\n",
    "del listing_nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Distribution of listing nulls by rowsource and bucket null percentages --RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Total columns by rowsource\n",
    "totals_byrowsource = listings.rowsource.value_counts()\n",
    "\n",
    "# Feature totals by rowsource\n",
    "non_complete_listings = listings.groupby('rowsource').count()\n",
    "\n",
    "# Feature totals by rowsource - totals, abs\n",
    "non_complete_listings = non_complete_listings.subtract(totals_byrowsource, axis=0).abs()\n",
    "\n",
    "# Get percentages of totals per rowsource\n",
    "non_complete_listings = non_complete_listings.div(totals_byrowsource, axis=0)\n",
    "\n",
    "# Melt data on rowsource\n",
    "non_complete_listings = non_complete_listings.reset_index().melt(id_vars='index')\n",
    "\n",
    "# Grouping by rowsource and provided column buckets\n",
    "grouping = ['index', pd.cut(non_complete_listings.value, bins=10)]\n",
    "non_complete_listings = non_complete_listings.groupby(grouping)\n",
    "non_complete_listings = non_complete_listings.size()\n",
    "non_complete_listings.plot.barh(title=\"Missing Value Couts per Percentage Buckets\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.isnull().sum().map({\n",
    "    0: 'Features with fully provided values'\n",
    "}).fillna('Features with missing values').to_frame('Status').groupby(\n",
    "    'Status').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We can see that missing values are scattered in this dataset. These missing values need to be investigated  individually and possibly in groups. \n",
    "\n",
    "Only 43 features have completed values and 53 of them have missing values.\n",
    "\n",
    "We can see that **license** and **squared_feet** are completely missing or significantly missing. For this reason we are removing these two features from the dataset and not wasting time analyzing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_drop_normal(column_list):\n",
    "    \"\"\"\n",
    "    Droping columns from listing without having to write the whole line.\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    column_list : column list to removed.\n",
    "    \"\"\"\n",
    "    listings.drop(column_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = ['license', 'square_feet']\n",
    "func_drop_normal(remove_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Reviews Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = read_concat(folder_names, 'reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.groupby('rowsource').count().plot.barh();\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left');\n",
    "for x in reviews.rowsource.value_counts().values:\n",
    "    plt.vlines(x, -1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: Simple. Reviews are all complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First 5 Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Availability Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.available.value_counts().plot.barh(\n",
    "    figsize=(10, 1), title='available', legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.groupby(['rowsource', 'available']).available.count(\n",
    "    ).plot.barh(figsize=(10, 1), title='available');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The Seattle segment have more counts than the boston segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Price Formats --C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.price.str.findall('\\D').value_counts().to_frame('price_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We will need to clean these invalid characters from the prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning prices\n",
    "calendar.price = cleaning_dollar(calendar.price)\n",
    "\n",
    "# Converting date to datetime\n",
    "calendar.date = pd.to_datetime(calendar.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating cleaning\n",
    "calendar.price.apply(type).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Line Trends by Dataset Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Trend with Filtered Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.groupby(['rowsource', 'date'])\\\n",
    "    .agg({'price':'sum'})\\\n",
    "    .reset_index()\\\n",
    "    .pivot(index='date', columns='rowsource', values='price')\\\n",
    "    .plot.line(figsize=(12, 10), lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The seattle segment started before the boston segment. Predictions outside each respective ranges could be invalid. Let's keep that in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Trend with Zeroed Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping missing values and provided values in a flag\n",
    "calendar['provided_flag'] = calendar.price.fillna('MissingValue')\n",
    "calendar.loc[calendar.provided_flag!='MissingValue', 'provided_flag'] = 'ProvidedValue'\n",
    "\n",
    "# Grouping source files with missing values\n",
    "calendar['source_group_flag'] = calendar.rowsource.str.cat(calendar.provided_flag, sep=' - ')\n",
    "\n",
    "# Aggregate by source/flag\n",
    "calendar_grp = calendar.groupby(['source_group_flag', 'date']).agg({'price':'sum'})\n",
    "calendar_grp = calendar_grp.reset_index()\n",
    "\n",
    "# Pivot table on source/flag to trend categories\n",
    "calendar_grp = calendar_grp.pivot(index='date', columns='source_group_flag', values='price')\n",
    "\n",
    "# Plot\n",
    "calendar_grp.plot.line(figsize=(12, 10), lw=3, alpha=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ranges where there are zero prices cover the whole range per dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Group/Flag Date Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "calendar.groupby('source_group_flag').agg({'date': ['min', 'max']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between Data Source Pricing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month tranformation functions\n",
    "month_func = lambda x: x.month\n",
    "delta_month_func = lambda x: x.max_date.shift(1).apply(\n",
    "    month_func) - x.max_date.apply(month_func)\n",
    "delta_days_func = lambda x: x.max_date.shift(1) - x.max_date\n",
    "\n",
    "calendar.groupby('rowsource').agg({\n",
    "    'date': 'max'\n",
    "}).rename(columns={\n",
    "    'date': 'max_date'\n",
    "}).assign(\n",
    "    delta_month=delta_month_func, delta_days=delta_days_func).fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis** We can see that there is more availability in Seattle listings. The total monthly trend prices are quite similar for both datasets. And the null values are scattered all within the min and max pricing dates. We are going to investigate these nulls during the listing null analysis.\n",
    "\n",
    "Dates of datasets do not match. This could mean that these segments were serviced at different times. ARBNB must have started servicing Seattle before Boston. However, Seattle has no data from 2017-01-02 to 2017-09-05, 8 months or 246 days. Therefore, as a warning prediction using Seattle during these ranges will be incorrect. The same applies to Boston."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Listings\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Object Features of Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculating missing percentages of total listings\n",
    "missing_pct = listings.isnull().sum() / listings.shape[0]\n",
    "\n",
    "# Listings examples with missing percentages\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "listings.select_dtypes(include=object).dropna(\n",
    "    axis=0, how='all').sample(1).T.merge(\n",
    "        missing_pct.to_frame('missing'),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='left').iloc[:, [1, 0]].sort_values(\n",
    "            'missing', ascending=False).style.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analying Numeric Features of Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.filter(regex='id')\n",
    "listings.select_dtypes(exclude=object).sample(4).T.merge(\n",
    "    missing_pct.to_frame('missing'),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='left').iloc[:, [-1, 0, 1, 2, 3]].sort_values(\n",
    "        'missing',\n",
    "        ascending=False).style.format(\"{:2,.2f}\").bar(subset=['missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Date Features of Listings --RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.select_dtypes(include=object)\\\n",
    "    .apply(pd.to_datetime, errors='ignore')\\\n",
    "    .select_dtypes(include=np.datetime64).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings['last_scraped'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We can see that we have to do some imputations on the reviews for the numerical fields. For the object fields, there are some numerical values that will have to be cleaned and some irrelevant columns such as URLs and names. One thing to notice is that **amenities** and **host_verification** can be split and converted to dummy variables. These could be helpful to the model.\n",
    "\n",
    "Perhaps the **calendar_last_scraped** feature has a meaningful link to the dataset. The other dates (**first_review, host_since, and last_review**) could be used to engineer new features. Only **last_scraped** seems to be irrelevant in this case since it only describes the date the datasets where extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_drop_normal('last_scraped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uderstanding Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Frist 5 Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Id Relationships Lenght Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.listing_id.astype(str).str.len()\\\n",
    "    .plot.box(vert=False, figsize=(10, 1), title=\"Calendar ID Len\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_func = lambda x: x.str.len()\n",
    "\n",
    "listings.filter(regex='id').select_dtypes(exclude=object).astype(str).apply(len_func)\\\n",
    "    .plot.box(vert=False, figsize=(10, 1), title=\"Listing IDs Len\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.filter(regex='id').apply(lambda x: x.astype(str).str.len())\\\n",
    "    .plot.box(vert=False, figsize=(10, 1), title=\"Reviews IDs Len\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We can surely see these are just individual reviews for the listings. Given that there are reviewer-ids with a length of 2 and 3 in the reviewers dataset and not in the listings dataset, we can assume that the listings don't include stays (missing records) for those host with 2 and 3 length-ids, but those hosts have reviews for the listings in the reviewer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The listing dataset id is the primary key that relates to the listing_id of the calendar dataset.\n",
    "* The scrape_id does not relate to anything in the downloaded dataset, so, therefore, it is irrelevant for the analysis. However, this id could be related to the calendar date since it could mean that the data was scraped during a specific calendar date.\n",
    "* The host_id in the listing dataset seems to be related to the reviewer_id of the reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calendar Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calendar Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "calendar_dup = calendar.groupby(\n",
    "    ['date', 'listing_id', 'rowsource',\n",
    "     'available']).size().to_frame('duplicates').query(\"duplicates>1\")\n",
    "calendar_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_price_dups = calendar_dup.reset_index().listing_id.unique()[0]\n",
    "print(f\"These are the id of price duplicates: \\n {calendar_price_dups}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Calendar Duplicates --RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conditions\n",
    "date_dup = calendar_dup.reset_index().date.astype(str).values.tolist()\n",
    "date_filter = calendar.date.astype(\n",
    "    str).apply(lambda x: True if x in date_dup else False)\n",
    "filter_cond = (calendar.listing_id == 12898806) & date_filter\n",
    "\n",
    "# Duplicate Examples\n",
    "calendar.loc[filter_cond, :].sort_values(['date', 'listing_id']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: There are 365 duplicates in the calendar dataset. These duplicates can be safely removed since they all share the same information. It does not really matter if we keep the first or last. We are removing these duplicates.\n",
    "\n",
    "The granularity of the table is at the following level:\n",
    "* Date\n",
    "* Listing Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag where there are no duplicates\n",
    "keep_first = ~calendar.duplicated(subset=['date', 'listing_id'], keep='last')\n",
    "\n",
    "# Remove duplicates by slicing\n",
    "calendar_clean = calendar[keep_first]\n",
    "\n",
    "# Calculate the difference beteween sizes\n",
    "dup_count = calendar_dup.count().values[0]\n",
    "dup_removed = calendar.shape[0] - calendar_clean.shape[0]\n",
    "\n",
    "print(f\"There where {dup_count} duplicates and {dup_removed} where removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listings Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 5 Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listings Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listings.groupby(['id']).size().to_frame('duplicates').query(\"duplicates>1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: Plain and simple, there are no duplicates in this dataset. Great!\n",
    "\n",
    "The granularity is at the id level. All other features are aggregates or normalized records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 5 Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reviews.groupby(['listing_id', 'reviewer_id', 'date',\n",
    "                 'id']).size().to_frame('duplicates').query(\"duplicates>1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.groupby(['id']).size().to_frame('duplicates').query(\"duplicates>1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Comments by Reviewer Relating to Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_duplicated = reviews.groupby([\n",
    "    'listing_id', 'reviewer_id', 'date'\n",
    "]).size().to_frame(\"duplicates\").query(\"duplicates>1\").reset_index()\n",
    "reviews_duplicated.style.bar(subset=['duplicates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Multiple Reviews per Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reviews.merge(reviews_duplicated.drop('duplicates', axis=1),\n",
    "              on=['listing_id', 'reviewer_id', 'date'], \n",
    "              how='left',\n",
    "              indicator=True)\\\n",
    "    .query(\"_merge=='both'\").head(6)\\\n",
    "    .style.background_gradient(subset=['reviewer_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: This dataset has no duplicates as well as listings. \n",
    "\n",
    "The granularity of the dataset is also at the id level. In this case the review id.\n",
    "\n",
    "However, as it relates to listings, there are several reviews for the same listing during the same date. Some of them have automated comments that could be impacting the overall listing reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Granulated at the listing_id, reviewer_id, and date level.\n",
    "* Some duplicates by automation. \n",
    "* Some manual duplicates by reviewer with different commets for same date and listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Relationships beteween Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between Geo Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get geo columns\n",
    "geo_columns = listings.select_dtypes(include=object).columns[-10:][:-6].tolist()\n",
    "\n",
    "listings[geo_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* space and street are too long for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing long features\n",
    "geo_columns.remove('space')\n",
    "geo_columns.remove('street')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mergin clean calendar with listings\n",
    "airbnb_geo_test = calendar_clean.merge(\n",
    "    listings[['id'] + geo_columns],\n",
    "    left_on='listing_id',\n",
    "    right_on='id',\n",
    "    how='left',\n",
    "    suffixes=['', '_list'],\n",
    "    indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_geo_test.rowsource.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_clean.rowsource.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_geo_test._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_geo_test.groupby(['state', 'rowsource']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del airbnb_geo_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We can see that the calendar dataset has full relationship with listing, which is excactly what we need. This means that all calendar listings_ids have relationships with listings ids.\n",
    "\n",
    "The merge worked, perfectly matching state and rowsource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for Meaningful Relationship Between Listing and Reivews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Reviews and Ids from Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing duplicate id in listings again\n",
    "print(\"There are\" , \n",
    "      listings.id.value_counts().where(lambda x: x>1).sum(),\n",
    "      \"dupliate ids in the listing dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "airbnb_review_test = listings.merge(\n",
    "    reviews, \n",
    "    left_on='id', \n",
    "    right_on='listing_id', \n",
    "    how='left', \n",
    "    suffixes=['', '_review'],\n",
    "    indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_review_test.groupby(['state', 'rowsource_review']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.groupby(['state', 'rowsource']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can see that reviews will have to be aggregated before joining to listings. But we will take care of that if it is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics About Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "comment_listing_counts = reviews.groupby('listing_id').size().to_frame(\n",
    "    'comment_count').groupby('comment_count').size().reset_index().rename(\n",
    "        columns={0: 'total_unique_listing_ids'})\n",
    "\n",
    "comment_listing_counts[['comment_count']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "comment_listing_counts.groupby(\n",
    "    pd.cut(comment_listing_counts.comment_count,\n",
    "           bins=list(np.arange(0, 550, 50)))).sum()[[\n",
    "               'total_unique_listing_ids'\n",
    "           ]].reset_index().set_index('total_unique_listing_ids').rename(\n",
    "               columns={'comment_count': 'comment_count_buckets'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can see that most listings have from 1 to 50 comments to apply natural language processing to predict ratings in the listing dataset if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship betwee Datasets Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: The datasets agree on the state and the source file name. The join between dataset is valid.\n",
    "\n",
    "The relationship between listing dataset reviews and reviews dataset comments is by the listing_id there are at most 474 reviews on at least one listing. Two listings have around 451 to 474 comments. However, the majority of the listings have 1 to 50 comments. So for some listing_ids predicting ratings might be difficult due to sparsity. \n",
    "\n",
    "The join for the final dataset is as follows:\n",
    "* listing_id on calendar\n",
    "* id on listings\n",
    "* inner join on calendar side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for Analysis --T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# column additions for scatter and monthly comparisons\n",
    "calendar_clean = calendar_clean.assign(date_num=lambda x: x.date.rank(method='dense'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the month name\n",
    "lmfunc_concat_name = lambda x: f\"{x.month:02d}\" + '-' + x.month_name() \n",
    "lmfunc_month_name = lambda x: x.date.apply(lmfunc_concat_name)\n",
    "\n",
    "calendar_clean =  calendar_clean.assign(month_name=lmfunc_month_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calendar Removals or Imputations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Assessing Calendar Removal Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_formats = {'row_count':'{:2,.0f}', 'pct_total':'{:2.2%}'}\n",
    "\n",
    "calendar_missing = calendar_clean.groupby('provided_flag').size().to_frame('row_count')\\\n",
    "        .assign(pct_total=lambda x: x.row_count/x.row_count.sum())\n",
    "\n",
    "calendar_missing.style.format(data_formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal Percentages by Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_rowsource_miss = calendar_clean.groupby(\n",
    "    ['rowsource', 'provided_flag']).size().to_frame('row_count').assign(\n",
    "        pct_total=lambda x: x.row_count / x.row_count.sum())\n",
    "\n",
    "calendar_rowsource_miss.style.format(data_formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removals by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_month_miss = calendar_clean.query(\"provided_flag=='MissingValue'\").groupby(\n",
    "    ['month_name', 'provided_flag']).size().to_frame('row_count').assign(\n",
    "        pct_total=lambda x: x.row_count / x.row_count.sum())\n",
    "\n",
    "sns.barplot(\n",
    "    x='pct_total', y='month_name', data=calendar_month_miss.reset_index());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Distributions by Month, RowSource, Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dataset groupings\n",
    "calendar_price_bymonth = calendar_clean.reset_index(drop=True).pivot(\n",
    "    columns='month_name', values='price')\n",
    "\n",
    "calendar_price_bysource = calendar_clean.reset_index(drop=True)\\\n",
    "    .pivot(columns='rowsource', values='price')\n",
    "\n",
    "# Title min max dates\n",
    "max_date = str(calendar_clean.date.max().date())\n",
    "min_date = str(calendar_clean.date.min().date())\n",
    "\n",
    "# Starting subplot\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(10, 20))\n",
    "\n",
    "plt.subplots_adjust(hspace=.3)\n",
    "\n",
    "calendar_price_bymonth.plot.hist(\n",
    "    bins=200,\n",
    "    logy=True,\n",
    "    alpha=.5,\n",
    "    title=f'Monthly Price (log) Distributions from {min_date} to {max_date}',\n",
    "    ax=ax[0])\n",
    "\n",
    "calendar_price_bysource.plot.hist(\n",
    "    bins=200,\n",
    "    logy=True,\n",
    "    alpha=.5,\n",
    "    title=f'Monthly Price (log) Distributions from {min_date} to {max_date}', ax=ax[1])\n",
    "\n",
    "plt.title('Prices Vs Dates Without Imputation')\n",
    "sns.scatterplot(x='date_num', \n",
    "                y='price', \n",
    "                hue='rowsource', \n",
    "                data=calendar_clean, ax=ax[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Relation with Availabity --RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Distribution of Prices by Availability\")\n",
    "sns.boxenplot(\n",
    "    x='available', y='price', data=calendar_clean.fillna(0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: Simply removing all null prices will remove 41.62% of the calendar data. Most of it will be from the Boston dataset. However, missing prices are related to unavailable listings or available = f (for False).\n",
    "\n",
    "As much as we want to prevent such drastic removals, we cannot do imputations for these nulls. We want to predict prices of listings and it is obvious that prices for unavailable listings are 0. For this reason, we are removing any null prices or all unavailable listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validator\n",
    "available_count = calendar_clean.query(\"available=='t'\").shape[0]\n",
    "print(f\"There are {available_count:2,.0f} available listings in the calendar dataset.\")\n",
    "\n",
    "# Removing all Nulls\n",
    "calendar_removals = calendar_clean.dropna()\n",
    "\n",
    "# Validating\n",
    "kept_count = calendar_removals.shape[0]\n",
    "print(f\"There are {kept_count:2,.0f} listings in the calendar dataset after removals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Other Irrelevant Columns --RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save date and date number for reference\n",
    "date_references = calendar_removals[['date', 'date_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['available', 'rowsource', 'provided_flag', \n",
    "                'source_group_flag', 'month_name', 'date']\n",
    "\n",
    "# Removing non-predictors\n",
    "try: calendar_removals = calendar_removals.drop(drop_columns, axis=1)\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing the Calendar Data Preparation and Releasing Objects from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy calendar removals to the completed calendar\n",
    "try:\n",
    "    calendar_complete = calendar_removals.copy()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "del calendar_clean, \n",
    "del calendar_removals, \n",
    "del calendar_dup, \n",
    "del calendar_grp,\n",
    "del calendar_missing, \n",
    "del calendar_month_miss, \n",
    "del calendar_price_bymonth,\n",
    "del calendar_price_bysource, \n",
    "del calendar_rowsource_miss\n",
    "del calendar_price_dups\n",
    "\n",
    "calendar_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Correlations of Final Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = AnalysisStatus(calendar_complete, listings)\n",
    "analysis.correlation_heatmap(['date_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Alone with price, the date number has at least a low correlation with price, which means that there is  $^2$  percentage of explenation by daily movements. The rest can be attributed to other features that we are going to explore next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing Removals or Imputations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listings Null Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.null_row_listingdist(30, 'Original Row Null Distribution')               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not observing an alarming rate of rows with null values across it. However, some of these features might be irrelevant for the analysis, and we should remove them. There could be other variables that can be impute. We will assess this distribution again after removals and imputations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target colmuns for imputations and transformations\n",
    "target_columns = listings.isnull().sum().where(lambda x: x>0).dropna().index.tolist()\n",
    "target_columns = ' '.join(target_columns)\n",
    "target_columns\n",
    "\n",
    "wordcloud = WordCloud(width=480, height=480, max_font_size=20, min_font_size=10).generate(target_columns)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these features in the wordcloud need to be analyized for null relations to other feature values. But some of theme are irrelevant for the analysis such as those that finish with url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns to Remove due to Irrelevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Irrelevant columns\n",
    "irrelevant_columns = listings.columns.str.contains('url|name|about|pic')\n",
    "irrelevant_columns = listings.columns[irrelevant_columns].tolist()\n",
    "\n",
    "# Show irrelevant features\n",
    "listings[irrelevant_columns].head(1).T.style.set_caption(\"Irrelevant Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These, above, are irrelevant columns since they describe web addresses and names. So there is no value to them in a regression analysis. \n",
    "\n",
    "On the contrary **host_has_profile_pic** and **require_guest_profile_picture** might add value to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Row Values Exceeding 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.null_row_feature_status(percentages=True, threshold=.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Transformations of Informational Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of Informational Columns with Missing Values --T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interest = ['']\n",
    "iform_arguments = 'access|over|rule|notes|space|summary|transit|interaction|host_about|desc'\n",
    "inform_columns = listings.filter(regex=iform_arguments).columns.tolist()\n",
    "listings[inform_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Values with 1 should match this\n",
    "listings[inform_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We are validating informational transformations to these null value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listings checkpoint\n",
    "listings_transform = listings.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting Transformations and Imputations\n",
    "provided_func = lambda x: x.isnull()\n",
    "inform_transform = listings_transform[inform_columns].where(provided_func, 0).fillna(1)\n",
    "\n",
    "# creating the new column for provided access information\n",
    "listings_transform.loc[:, inform_columns] = inform_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching 1 to provided information\n",
    "listings_transform[inform_columns].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tranformation checks out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Correlations of Informational Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = AnalysisStatus(calendar_complete, listings_transform)\n",
    "analysis.correlation_heatmap(inform_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Null Distributions After Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.null_row_listingdist(\n",
    "    bins=30, title='Null Distribution After Informational Tranformations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Informational Feature Transformation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These informational columns are not substantially correlated with price. For this reason, they will be removed from the dataset, with the exception of **house_rules** or **interaction**. They have the biggest correlation with price. However, they are also heavily correlated with each other. For this reason, we are going to pick just **house_rules** because it has the highest correlation with price.\n",
    "\n",
    "We are also removing description since it had no nulls and by itself it provides no value given that the columns have too much categories of text.\n",
    "\n",
    "We now have less row nulls than before. It is looking much cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_drop_transform(column_list):\n",
    "    \"\"\"\n",
    "    Droping columns from listing without having to write the whole line.\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    column_list : column list to removed.\n",
    "    \"\"\"\n",
    "    listings_transform.drop(column_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = [x for x in inform_columns if x != 'house_rules']\n",
    "func_drop_transform(remove_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.null_row_feature_status(percentages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Dolar Value Features --C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preparing for tranformations\n",
    "clean_func = lambda x: cleaning_dollar(x)\n",
    "dollar_columns = listings_transform.columns.str.contains('fee|price|deposit|extra')\n",
    "dollar_columns = listings_transform.columns[dollar_columns].tolist()\n",
    "\n",
    "listings_transform.loc[:, dollar_columns] = listings_transform[\n",
    "    dollar_columns].apply(clean_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Correlations for Dollar Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = AnalysisStatus(calendar_complete, listings_transform)\n",
    "analysis.correlation_heatmap(dollar_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We can see that monthly price is highly correlated with price. But these two variables are linking variables, which means that manipulating one will manipulate the other one dependently. Therefore, monthly_price is not independent. We must remove it. The same applies to weekly_price.\n",
    "\n",
    "So there is no use for using monthly_price and weekly_price to predict price because that is a bit redundant--no wonder they are heavily correlated. However, cleaning_fee and security_deposit are the main drivers of the listing price. Thus, we are dropping listing price derived features and keeping the fees, which are additions to the price.\n",
    "\n",
    "The null rows are mostly clean at this time. We are doing a bit more cleaning, and we will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_dollarder = ['monthly_price', 'weekly_price']\n",
    "func_drop_transform(drop_dollarder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputations of Zero Fees and Deposits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fee_columns = ['cleaning_fee', 'security_deposit']\n",
    "listings_transform.loc[:, fee_columns] = listings_transform[fee_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Null Distributions After Removal of Pricing Derived Features and Imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.null_row_listingdist(\n",
    "    bins=30, title='Null Distribution After Pricing Derived Removals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It is looking very clean now. But we still have more work to do--more cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.null_row_feature_status(percentages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Missing Geographical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Sample Rows of Geographical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Geo with nulls\n",
    "geo_null_cololumns = [\n",
    "    'jurisdiction_names', 'neighbourhood_group_cleansed', 'neighbourhood',\n",
    "    'host_neighbourhood', 'host_location', 'city'\n",
    "]\n",
    "\n",
    "# Geo complete\n",
    "geo_comp_columns = ['state', 'street']\n",
    "\n",
    "listings_transform[geo_null_cololumns + geo_comp_columns].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jurisdiction and State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_transform.fillna('missing').groupby(['state', 'jurisdiction_names']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing state\n",
    "listings_transform.loc[:, 'state'] = listings_transform.state.apply(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighbourhood Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "match_func = lambda x: x.neighbourhood == x.host_neighbourhood\n",
    "listings_transform[[\n",
    "    \"neighbourhood_group_cleansed\", \"neighbourhood\", \"host_neighbourhood\",\n",
    "    \"host_location\", \"neighbourhood_cleansed\"\n",
    "]].astype(str).assign(match=match_func).query(\"match==False\").sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking sparsity\n",
    "listings_transform.city.value_counts().plot.barh(figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sparse Classes for City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing sparsity issue\n",
    "eng_group_bos = {True:'Boston'}\n",
    "eng_group_sea = {True:'Seattle'}\n",
    "eng_group_city = lambda x: x.str.contains('Boston|Seattle')\n",
    "\n",
    "# Grouping Seattle Areas\n",
    "fill_sea = listings_transform.city.str.contains('seattle', case=False)\\\n",
    "    .map(eng_group_sea)\n",
    "\n",
    "# Grouping Boston Areas\n",
    "fill_bos = listings_transform.city.str.contains('boston', case=False)\\\n",
    "    .map(eng_group_bos)\n",
    "\n",
    "# Grouping Other Areas\n",
    "group_sparse = fill_bos.fillna(fill_sea).where(eng_group_city, 'Other')\n",
    "\n",
    "# Applying sparcity fix to city\n",
    "listings_transform.loc[:, 'city'] = group_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting sparsity\n",
    "listings_transform.city.value_counts().plot.barh(\n",
    "    figsize=(10, 1), title='Sparsity after Grouping Classes')\n",
    "\n",
    "# Checking sparsity\n",
    "(listings_transform.city.value_counts()/listings_transform.city.count())\\\n",
    "    .to_frame().style.format(\"{:2.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We found that **jurisdiction_names** are related identically to state and are completely missing for Massachusetts. Thus we are removing it. Also **neighbourhood_group_cleansed** has too many missing values to map with zipcodes to extract the neighborhood. We could get a dataset to map this. But for now, we will just remove it. There are some free zip code databases online that we can use to extract these. However, although **neighbourhood** has some duplicates and the process to extract those using zipcodes is not set yet. We are keeping this feature as predictor and converting the nulls to dummies.\n",
    "\n",
    "We are removing **jurisdiction_names, neighbourhood_group_cleansed, and neighbourhood** from the listings and adding them to a tryout dataframe so we can pass it to the model for curiosity's sake. \n",
    "\n",
    "As for **city**, even though the nulls where insignificant, the sparsity of city groups were more concentrated in Boston and Seattle. We can use this feature but we need to group these sparse classes into other. However, some of these cities pertains to Boston, so we will need to map those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['jurisdiction_names', 'neighbourhood_group_cleansed', 'neighbourhood']\n",
    "tryout = listings_transform[drop_columns]\n",
    "func_drop_transform(drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.null_row_feature_status(percentages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing Has Availability Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of zero prices\n",
    "listings_transform.query(\"has_availability!=has_availability\").price.where(\n",
    "    lambda x: x == 0).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know available listings have prices. All listings have prices in the listings dataset. Therefore, has_availability should all be True. In this case, this feature provides no value to the model, and we will remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_drop_transform('has_availability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews where Number of Reviews where not Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns related to reviews\n",
    "review_columns = listings_transform.columns[\n",
    "    listings_transform.columns.str.contains('review')]\n",
    "review_columns = ['id'] + review_columns.tolist()\n",
    "\n",
    "listings_transform[review_columns].query(\n",
    "    \"number_of_reviews!=0\").isnull().sum().to_frame('missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Where numer of reviews are not zero there are minimal null values that can be drop from the model, or just converted to the median rating for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews where Number of Reviews where Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_transform[review_columns].query(\"number_of_reviews==0\").isnull().sum(\n",
    ").to_frame('missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All of these values have a meaning of no rating provided. Therefore, we should not remove any of these. Perhaps, what we can do is create another feature stating that the zero provided is actually a non provided field istead of an actual zero rating. The new feature will be called non_provided_rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews where Number of Reviews where not Zero Including Reviews Dataset Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_transform[review_columns].query(\"number_of_reviews!=0\").merge(\n",
    "    reviews.groupby('listing_id', as_index=False).sum(),\n",
    "    left_on='id',\n",
    "    right_on='listing_id',\n",
    "    how='left',\n",
    "    suffixes=['', '_reviews']).isnull().sum().to_frame('missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are comment that we could use to extract ratings using NLP. However, the extra work might not provide us the accuracy that we deserve given that there is so few missing ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_provided = listings_transform.filter(regex='score|review').drop(\n",
    "    'number_of_reviews', axis=1).isnull().sum(axis=1)\n",
    "\n",
    "non_provided.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The null reviews are for a whole row of ratings per specific listings. We can see that all rows have either 10 null rows or 0 rows nulls for specific listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revie columns of interest\n",
    "review_col = listings_transform.filter(regex='score|review').drop(\n",
    "    'number_of_reviews', axis=1).columns.tolist()\n",
    "\n",
    "# Transforming reviews where whole row is null\n",
    "listings_transform = listings_transform.assign(\n",
    "    non_provided_rating=non_provided.map({0:0, 10:1}))\n",
    "\n",
    "# Filling all nulls to zero since they mean not provided reviews\n",
    "listings_transform.loc[:, review_col] = listings_transform[review_col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the review transformations where number of reviwes is zero\n",
    "listings_transform[review_columns].query(\"number_of_reviews==0\").isnull().sum(\n",
    ").to_frame('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Validating firts_review and provided\n",
    "listings_transform[review_columns +\n",
    "                   ['non_provided_rating']].query(\"first_review!=0\")[[\n",
    "                       'first_review', 'non_provided_rating'\n",
    "                   ]].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating first review and not provided\n",
    "listings_transform[review_columns +\n",
    "                   ['non_provided_rating']].query(\"first_review==0\")[[\n",
    "                       'first_review', 'non_provided_rating'\n",
    "                   ]].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The transformation worked. Let's see the correlation of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = AnalysisStatus(calendar_complete, listings_transform)\n",
    "score_col = listings_transform.filter(regex='score').columns.tolist()\n",
    "analysis.correlation_heatmap(['non_provided_rating'] + score_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We can see that scores bring no value to the model. Therefore, we will remove all of these fields even though we still have null values for reviews that have number of ratings other than zero. This is because there is no chance that imputations will increase the insignificant correlations this score features provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_drop_transform(score_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **Engineering review_lag and review_rate** --E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    checkpoint\n",
    "except:\n",
    "    checkpoint = listings_transform.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the lag between first and last review\n",
    "day_func = lambda x: x.days\n",
    "diff_func = lambda x: (pd.to_datetime(x.last_review) - pd.to_datetime(\n",
    "    x.first_review)).apply(day_func)\n",
    "reate_func = lambda x: (x.review_lag / x.number_of_reviews)\n",
    "\n",
    "# engineering review lag\n",
    "listings_transform = listings_transform.assign(review_lag=diff_func)\n",
    "\n",
    "# engineering review rate\n",
    "listings_transform = listings_transform.assign(review_rate=reate_func)\n",
    "\n",
    "# dropping fist_review and last_review\n",
    "review_dates = ['first_review', 'last_review']\n",
    "listings_transform.drop(review_dates, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero for no review provided\n",
    "listings_transform.loc[:, 'review_rate'] = listings_transform.review_rate.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = AnalysisStatus(calendar_complete, listings_transform)\n",
    "analysis.correlation_heatmap(\n",
    "    ['review_lag', 'review_rate', 'number_of_reviews', 'reviews_per_month'] +\n",
    "    ['non_provided_rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We are removing **first_review** and **last_review** and replacing it with **review_lag** to account for the dates between the first review and the last review. We are also engineering the rate of reviews by dividing the **review_lag** by the **number_of_reviews**, this is the **review_rate**. But we can see that the review_lag provided no value to the model. So we will remove. \n",
    "\n",
    "The same way the **review_rate** ended up providing less value to the model than its derived feature, **number_of_reviews**. Thus, we are removing **review_rate**.\n",
    "\n",
    "Also, the newly created feature **non_provided_rating** adds no value to the model. So we will remove it too.\n",
    "\n",
    "**Engineering Description**:\n",
    "* For the lag, we subtracted the first review date from the last review date.\n",
    "* For the review rate, we divided the review lag by the number of reviews, which give us the rate of reviews between the first review date and the last review date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the only valuable features are the **number_of_reviews** and **reviews_per_month**.\n",
    "\n",
    "The **review_lag** is insignificant. So we will remove it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_drop_transform([\n",
    "    'review_lag', 'review_rate',\n",
    "    'non_provided_rating'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing Other Irrelevant Features** --RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_columns.remove('host_about')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_columns = ['host_has_profile_pic', 'require_guest_profile_picture']\n",
    "\n",
    "# Remove irrelevant columns\n",
    "drop_columns = [\n",
    "    i for i in irrelevant_columns\n",
    "    if i not in possible_columns + ['jurisdiction_names']\n",
    "]\n",
    "\n",
    "func_drop_transform(drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We ended up removing all irrelevant features except for  the picture related columns, which could be benefetial to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host Acceptance and  Response Rate and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning percentages\n",
    "listings_transform.loc[:, 'host_acceptance_rate'] = cleaning_percent(\n",
    "    listings_transform.host_acceptance_rate)\n",
    "\n",
    "listings_transform.loc[:, 'host_response_rate'] = cleaning_percent(\n",
    "    listings_transform.host_response_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding prefix\n",
    "lmfunc_colname = lambda x: 'resp_t_' + str(x)\n",
    "\n",
    "response_time = pd.get_dummies(\n",
    "    listings_transform.host_response_time,\n",
    "    dummy_na=True).rename(columns=lmfunc_colname)\n",
    "\n",
    "# Extracting prefixed columns\n",
    "response_time_col = response_time.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add newly created dummies for response time and delete original\n",
    "listings_transform = pd.concat([listings_transform, response_time],\n",
    "                               axis=1).drop(\n",
    "                                   'host_response_time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis = AnalysisStatus(calendar_complete, listings_transform)\n",
    "analysis.correlation_heatmap(['host_acceptance_rate', 'host_response_rate'] +\n",
    "                             response_time_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: As we can see, **host_response_time** adds little value to the model. Thus, we will remove it. Also, **host_response_rate** adds no value to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_drop_transform(['host_response_rate'] + response_time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.null_row_feature_status(percentages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_transform.host_acceptance_rate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_transform.host_acceptance_rate.plot.box(vert=False, figsize=(5, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputating median to acceptance reate\n",
    "resp_rate = listings_transform.host_acceptance_rate.median()\n",
    "listings_transform.loc[\n",
    "    :, 'host_acceptance_rate'] = listings_transform.host_acceptance_rate.fillna(\n",
    "                           resp_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = AnalysisStatus(calendar_complete, listings_transform)\n",
    "analysis.correlation_heatmap(['host_acceptance_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We can see that the acceptance rate did provide value to the model. This feature had null values that where imputed with the median. The explanation is that in general, 50% of all the acceptance rates fall between or in 100%. Thus, the middle of the distribution is 100%--Kudos to AIRBNB and their hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Nulls at the Row Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.null_row_listingdist(30, 'Null at Row Level After All Removals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting, Engineering, and Removing amenities and host_verifications --E RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiationg encoder\n",
    "ds = DummySplit(listings_transform)\n",
    "\n",
    "# Original example\n",
    "listings_transform[['amenities']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split add and drop -- save columns and example of transformations\n",
    "ds.split_create_dummy('amenities')\n",
    "ds.add_dummies(drop_original=True)\n",
    "amenities_columns = ds.dummy_columns[:]\n",
    "ds.dummy_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original example\n",
    "listings_transform[['host_verifications']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split add and drop -- save columns and example of transformations\n",
    "ds.split_create_dummy(feature_name='host_verifications')\n",
    "ds.add_dummies(drop_original=True)\n",
    "host_verifications = ds.dummy_columns[:]\n",
    "ds.dummy_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release memory\n",
    "del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Correlation of New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = AnalysisStatus(calendar_complete, listings_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(amenities_columns, show_values=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = ['AirConditioning', 'FamilyKidFriendly', 'CableTV', 'TV', 'ElevatorinBuilding']\n",
    "analysis.correlation_heatmap(scope, show_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(host_verifications, show_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: In the amenities we found that some features stand out with moderate correlations--AirConditioning, FamilyKidFriendly, CableTV, TV, and ElevatorinBuilding were some of them. These features will add values to the model. However, they are a slightly correlated with each other, except for FamilyKidFriendly and ElevatorinBuilding.\n",
    "\n",
    "On the contrary, host_verividations did not have any valuable features extracted, except for amex, which scored a $r$ of .10. Perhaps, we can add it to the model at the end.\n",
    "\n",
    "We are adding to the model the highest correlated features that have the lowest multicollinearity with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Transformations and Removals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Removing Irrelevant Columns** -- RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removals = [\n",
    "    'calendar_updated', 'experiences_offered', 'is_location_exact',\n",
    "    'rowsource', 'zipcode', 'street', 'host_location', 'smart_location',\n",
    "    'host_neighbourhood', 'neighbourhood_cleansed', 'country', 'country_code',\n",
    "    'scrape_id'\n",
    "]\n",
    "\n",
    "listings_transform.drop(removals, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations of t/f Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = [\n",
    "    'host_has_profile_pic', 'host_identity_verified', 'host_is_superhost',\n",
    "    'instant_bookable', 'require_guest_phone_verification', 'requires_license',\n",
    "    'require_guest_profile_picture'\n",
    "]\n",
    "\n",
    "listings_transform.loc[:, transform] = listings_transform[transform].apply(\n",
    "    lambda x: x.map({\n",
    "        'f': 1,\n",
    "        't': 0\n",
    "    }))\n",
    "\n",
    "listings_transform[transform].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Engieneering host_lifetime from  host_since and calendar_updated** -- E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engieneering = ['host_since', 'calendar_last_scraped']\n",
    "\n",
    "# Engineering feature by subracting\n",
    "host_lifetime = listings_transform.calendar_last_scraped.apply(\n",
    "    pd.to_datetime) - listings_transform.host_since.apply(pd.to_datetime)\n",
    "\n",
    "# Extracting date number\n",
    "host_lifetime = host_lifetime.apply(lambda x: x.days)\n",
    "\n",
    "# Deleting original from dataset\n",
    "listings_transform.drop(engieneering, axis=1, inplace=True)\n",
    "\n",
    "# Adding engineered feature to dataset\n",
    "listings_transform['host_lifetime'] = host_lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = AnalysisStatus(calendar_complete, listings_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(['host_lifetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listings Engineered Feature Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A correlation of .1 is a small correlation. However, it could be beneficial to the model by a small margin. We will test the model with the highest correlated features first and will add some like this one to it to attempt to improve its scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = listings_transform.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Encoder to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Saving Categorical for further analysis\n",
    "df_categorical = listings_transform.select_dtypes(include=object)\n",
    "\n",
    "# Example of categorical\n",
    "df_categorical.sample(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying One-Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initiating Encoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Get only columns with categories\n",
    "listings_object = listings_transform.select_dtypes(\n",
    "    include=object)\n",
    "\n",
    "# Get list of removal columns\n",
    "remove_original = listings_object.columns.tolist()\n",
    "\n",
    "# Fitting and tranforming categories\n",
    "# Fill nulls with missing to create missing dummy\n",
    "X_encoded = encoder.fit_transform(listings_object.astype(str))\n",
    "\n",
    "# Remove original encoded columns\n",
    "listings_transform.drop(remove_original, axis=1, inplace=True)\n",
    "\n",
    "# Encoded coluns\n",
    "columns_encoded = encoder.categories_\n",
    "\n",
    "# Extracting from list of list\n",
    "dummy_columns = []\n",
    "for original, enc_col in list(zip(remove_original, columns_encoded)):\n",
    "    for dummy in enc_col:\n",
    "        dummy_columns.append(re.sub('\\W', '', (original + '_' + dummy)))\n",
    "\n",
    "# Creating dataframe\n",
    "X_encoded = pd.DataFrame(X_encoded.toarray(), columns=dummy_columns)\n",
    "\n",
    "# Applying listings \n",
    "listings_encoded = pd.concat([listings_transform, X_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_analysis_encoded():\n",
    "    \"\"\"\n",
    "    Re-runs the analysis class and updates its dataframe objects.\n",
    "    It prepares for correlation status.\n",
    "    \"\"\"\n",
    "    analysis.listings = listings_encoded\n",
    "    analysis._merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_get_dummies(drop_list):\n",
    "    \"\"\"\n",
    "    Drop dummies from transformed listings, refreshes the analysis, and\n",
    "    updates the dummy_columns variable to use in correlation map.\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    drop_list : the list of dummy features to drop from the analysis\n",
    "    \"\"\"\n",
    "    global dummy_columns\n",
    "    analysis.df_merged.drop(drop_list, axis=1, inplace=True)\n",
    "    # refresh_analysis_encoded()\n",
    "    dummy_columns = [x for x in dummy_columns if x not in drop_list]\n",
    "\n",
    "def get_dummy_from_feature(feature_name):\n",
    "    \"\"\"\n",
    "    Extracts the dummies relative to the feature from which it was created.\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    feature_name : the feature to extract dummies from the trasformed listings\n",
    "    \"\"\"\n",
    "    return [x for x in dummy_columns if feature_name in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_analysis_encoded()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Perfect Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis.colliniearity_table(include_perfect=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies(['id', 'listing_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Market Vs. State Correlations with Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Get target dummies\n",
    "market_dummies = df_categorical.market.astype(\n",
    "    str).apply(lambda x: 'market_' + re.sub('\\W', '', x)).unique().tolist()\n",
    "\n",
    "# Show correlation of target dummies only\n",
    "# Adding State\n",
    "analysis.correlation_heatmap(market_dummies + ['state_MA', 'state_WA'], show_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.market.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We can see there are some correlated fields. However, San Fransisco market stands out as having no correlation. Hence, there is low sparsity in San Fransisco.\n",
    "\n",
    "Since markets other than Boston and Seattle are not well correlated with price, we are going to just drop market. Boston and Seattle markets are fully correlated with its state as it should. Thus, we are using state as a predictor instead of market.\n",
    "\n",
    "Another interesting finding is state perfect multicollinearity. This means that I just can drop one of them and rename the other one state: 1 for Boston and 0 for Seattle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies(market_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making State One Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate with this\n",
    "analysis.df_merged[[\n",
    "    'state_MA', 'state_WA'\n",
    "]].melt().where(lambda x: x.value == 1).dropna().variable.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping Washington from state\n",
    "drop_get_dummies(['state_WA'])\n",
    "\n",
    "# Changing name\n",
    "analysis.df_merged.rename(columns={'state_MA':'stateMApWAn'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis.df_merged.stateMApWAn.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing Host Listing Count Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(['host_listings_count', 'host_total_listings_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: It does not matter wich one we remove, they are both correlated the same way with calendar listing price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies(['host_total_listings_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding High Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.colliniearity_table(multi_r=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bed Vs. Accommodates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(['beds', 'accommodates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We see a high indication of multicollinearity with beds and accommodates. It make sense to think that a listing with more beds accommodates more people. However, accommodates have more corellation with calendar listing price than beds. For this reason, we are removing beds from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies('beds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Availability Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_cond = analysis.df_merged.columns.str.contains('avail')\n",
    "available_col = analysis.df_merged.columns[available_cond].tolist()\n",
    "analysis.correlation_heatmap(available_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: All availability features are correlated with each other, except for availability_365. Nevertheless, it has an insignificant correlation with price. Thus, we are removing it. The other 3 (availability_30, availability_60, and availability_90) may be all calculated features and therefore have multicollinearity because of it. Base on the correlation with calendar listing price, we are not keeping any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies(available_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Host Listings Vs. Calculated Host Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(['host_listings_count', 'calculated_host_listings_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: Giving that host_listings is heavily correlated with its calculated counterpart, we are keeping only the feature most correlated with calendar listing price, which is **host_listings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies('calculated_host_listings_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Price in Listing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to analyize this. It is obvious that last sccraped price in listing will be slighly correlated with calendar listing price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies('price_listing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Internet and WirelessInternet Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(['Internet', 'WirelessInternet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: They have almost the same correlation. However, removing one of them might create some bias since they mean exactly the same thing. Instead of removing it, we are just combining them into one.\n",
    "\n",
    "The methodology is summing both dummies. If the value is zero, then no internet was provided. If not zero, then internet was provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the original shape of the model data\", analysis.df_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condensing dummie into one series\n",
    "internet_condense = analysis.df_merged[['Internet', 'WirelessInternet'\n",
    "                    ]].sum(axis=1).where(lambda x: x==0, 1)\n",
    "\n",
    "internet_condense.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Where sum of both is greater than zero, or at least one has internet\n",
    "analysis.df_merged[['Internet', 'WirelessInternet'\n",
    "                    ]].eval(\"Internet+WirelessInternet>0\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where there is a one on either of them, validation\n",
    "analysis.df_merged.query(\"Internet==1 or WirelessInternet==1\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies('WirelessInternet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_merged.loc[:, 'Internet'] = internet_condense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(['Internet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iron and HairDryer Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(['Iron', 'HairDryer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: Since these two dummy variables most likely are offered one without the other, they have a high multicollinearity. However, we could engineer a new feature called iron_hairdryer to see if it performs better than the individual ones.\n",
    "\n",
    "The methodology of the new feature (iron_hairdryer) is if the sum of both is 2, then they provided both iron and hair dryer--we set it to 1. If the sum is not 2, then they did not provided the combo and therefore is set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combining into one series\n",
    "combination_ind = analysis.df_merged[[\n",
    "    'Iron', 'HairDryer'\n",
    "]].sum(axis=1).where(lambda x: x == 0, 1)\n",
    "\n",
    "combination_ind.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where sum of both is greater than zero, or at least one has internet\n",
    "analysis.df_merged[['Iron', 'HairDryer'\n",
    "                    ]].eval(\"Iron+HairDryer>0\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where there is a one on either of them, validation\n",
    "analysis.df_merged.query(\"Iron==1 or HairDryer==1\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_merged.loc[:, 'iron_hairdryer'] = combination_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(['Iron', 'HairDryer', 'iron_hairdryer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: Unexpected, but the combination of both did not added value to the model. We will only keep Iron for the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies(['iron_hairdryer', 'HairDryer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City Vs. State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(['city_Seattle', 'city_Boston', 'city_Other', 'stateMApWAn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We have the same causal multicollinearity than state vs. market since one is the parent of the other. Since we have converted state already, we are keeping state, even though Boston city has more correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies(['city_Seattle', 'city_Boston', 'city_Other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private Room, Apartment, Entire Home, and House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.colliniearity_table(multi_r=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies(['longitude', 'latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_get_dummies(['host_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: \n",
    "Longitude and latitude will not provide us valuable information since it has too many classes. \n",
    "\n",
    "Also, given that the property type and room type multicollinearity don't seem to be causal in nature. We are keeping these features as they are in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Property Type Vs. Room Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_type = get_dummy_from_feature('room_type')\n",
    "property_type = get_dummy_from_feature('property_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(room_type, show_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.property_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We can see a very high sparsity in the property type feature classess. We will have to group some categories to make it more even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping columns\n",
    "property_keep_col = ['property_type_Apartment', 'property_type_House']\n",
    "\n",
    "# other columns\n",
    "property_other_col = analysis.df_merged.filter(regex='property_type').drop(\n",
    "    property_keep_col, axis=1).columns.tolist()\n",
    "\n",
    "# Compact property type\n",
    "property_other = analysis.df_merged[property_other_col].sum(axis=1)\n",
    "\n",
    "drop_get_dummies(property_other_col)\n",
    "\n",
    "analysis.df_merged.loc[:, 'property_type_other'] = property_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.correlation_heatmap(property_keep_col + ['property_type_other'], show_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: House and apartment have multicollinearity and the high correlation goes to house. So basically by choosing to list an apartment or a house, we will not see a significant difference in price. Thus, each one of them will do well in the model We will remove apartment.\n",
    "\n",
    "Private room and entire home are correlated as well. But entire apartment is the biggest. So we are removing private room following the same logic as before.\n",
    "\n",
    "The rest looks ok. But the grouped property type other provides no value to the model. We can remove this one. So for property type we can say if it is a house, it will have a higher price than any other. If it is an apartment, it will follow the same logic since it is similarly correlated with price and with apartment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['property_type_Apartment', 'room_type_Privateroom', 'property_type_other']\n",
    "drop_get_dummies(drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Final Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis.df_merged.isnull().sum()/analysis.df_merged.shape[0]).where(lambda x: x>0).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: Given that the null values in the final frame is miniscule, we are safe to remove all of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_merged.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality by Removing Low Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airbnb = analysis.df_merged\n",
    "del analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the count of nulls\n",
    "df_airbnb.drop('row_null_pct', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute collinearities with price\n",
    "collinearity_price = df_airbnb.corr()[['price']].drop('price', axis=0).apply(abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Keeping only 5% and above\n",
    "reduced_features = collinearity_price.sort_values(\n",
    "    'price', ascending=False).where(lambda x: x > .0).dropna().index\n",
    "\n",
    "reduced_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif(dataframe):\n",
    "    \"\"\"\n",
    "    Creates a series of variance inflation factors to determine their multicollinearity.\n",
    "    parameter\n",
    "    ---------\n",
    "    dataframe : the final reduced dataset with substantial explanatory features correlated with\n",
    "    dependent feature.\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    \n",
    "    # ignore divide by zero warning\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    X = add_constant(dataframe)\n",
    "    columns = X.columns\n",
    "    \n",
    "    # Factor container\n",
    "    vif_factors = []\n",
    "        \n",
    "    shape_idx = X.shape[1]\n",
    "    \n",
    "    # For all features\n",
    "    for i in range(shape_idx):\n",
    "        print(f\"[{'='*i}]{i/(shape_idx-1):2.2%}\", flush=True, end='\\r')\n",
    "        \n",
    "        vif_factor = variance_inflation_factor(X.values, i)\n",
    "            \n",
    "        # Factor for the feature\n",
    "        vif_factors.append(vif_factor)\n",
    "              \n",
    "    warnings.filterwarnings(\"default\")\n",
    "        \n",
    "    return pd.Series(vif_factors, index=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vif_factors = vif(df_airbnb[reduced_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We don't see any issues with mutlicollinearity, except for some dummy features. This is not necessarliy a problem. We are continuing the process with these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airbnb.price.plot.box(vert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ordering dataset chronologically\n",
    "df_airbnb = df_airbnb.query(\"price < 1500\").sort_values('date_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airbnb.price.plot.box(vert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(x='date_num', y='price', hue='stateMApWAn', data=df_airbnb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependable variable\n",
    "y = df_airbnb.loc[:, 'price']\n",
    "\n",
    "# explanatory features\n",
    "X = df_airbnb[reduced_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split at 20% \n",
    "test_size = int(round(X.shape[0]*.20, 0))\n",
    "train_size = X.shape[0] - test_size\n",
    "\n",
    "print('Test Size: ', test_size, '\\nTrain Size:', train_size)\n",
    "\n",
    "# Train Set\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "# Test Set\n",
    "X_test = X[train_size:]\n",
    "y_test = y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.Series(lr.coef_, index=X.columns).to_frame('coefficients').style.bar(align='mid').format('{:2.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclussion"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "1207px",
    "width": "457px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "2237px",
    "left": "27px",
    "top": "140px",
    "width": "522px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
